[{"content":"\rThis project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck.\rWe all know the story behind the famous titanic ship. But just incase you don't know the story behind Titanic, let me give you a brief gist üòâ.\rTitanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time.\rTitanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers.\rTitanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier.\rOn the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised.\rOver about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\rLet's dive into the brief steps of how I handled this project using anaconda jupyter notebook and tableau. We will get the data, explore, process, and build the ML models. We will then depoly the model using TabPy function and see the results in tableau.\rYou can access the datasource here - [Titanic](https://www.kaggle.com/c/titanic) dataset on Kaggle. The demographic characteristics are the independent variables.\rStep 1: I got data from Kaggle. You can access it here You can download the dataset here. I performed some basic data exploration steps to review the data.\n1 train_df.info() 1 train_df.describe() From this result, while other feature counts are 891, the count of age is 714 depicting missing data. 38% of the passengers out of the training set survived the titanic (mean - 0.38) with ages ranging from 0.4 to 8.\n1 train_df.head(10) There is need to convert most features to numeric so the model can process the features accurately.\nTo take a deeper look at the missing data, I ran this short code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Calculate missing values per column missing_total = train_df.isnull().sum() # Calculate percentage of missing values missing_percent = (missing_total / train_df.shape[0]) * 100 # Combine into a single DataFrame missing_data = pd.DataFrame({ \u0026#39;Total\u0026#39;: missing_total, \u0026#39;%\u0026#39;: missing_percent.round(1) }) # Sort by total missing values and show top 5 missing_data_sorted = missing_data.sort_values(by=\u0026#39;Total\u0026#39;, ascending=False) missing_data_sorted.head(5) To clean the data and fill missing columns, I used the part of the code block # Fill NAN values in Age with the random numbers generated in step 2\nThe result Step 2: There is need to create and deploy the function that can take the selected parameter values in Tableau as input and return the probability of a person surviving Titanic. I wrote the final Python code as seen below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # Importing the libraries import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder import tabpy_client from tabpy_client.client import Client # Loading the dataset train_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/train.csv\u0026#39;) test_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/test.csv\u0026#39;) # Dealing with missing values in the Age variable data = [train_df, test_df] for dataset in data: mean = train_df[\u0026#34;Age\u0026#34;].mean() std = train_df[\u0026#34;Age\u0026#34;].std() # Use train_df.std() for consistency is_null = dataset[\u0026#34;Age\u0026#34;].isnull().sum() # Compute random numbers between mean, std, and is_null rand_age = np.random.randint(mean - std, mean + std, size=is_null) # Fill NAN values in Age with the random numbers generated age_slice = dataset[\u0026#34;Age\u0026#34;].copy() age_slice[np.isnan(age_slice)] = rand_age dataset[\u0026#34;Age\u0026#34;] = age_slice dataset[\u0026#34;Age\u0026#34;] = train_df[\u0026#34;Age\u0026#34;].astype(int) # Define age bins and labels bins = [0, 11, 19, 27, 35, 43, 58, 66, np.inf] labels = [0, 1, 2, 3, 4, 5, 6, 7] # Apply pd.cut to categorize Age variable for dataset in data: dataset[\u0026#39;Age\u0026#39;] = pd.cut(dataset[\u0026#39;Age\u0026#39;], bins=bins, labels=labels, right=False).astype(int) #Encoding the categorical labels le = LabelEncoder() for dataset in data: dataset[\u0026#39;Sex\u0026#39;] = le.fit_transform(dataset[\u0026#39;Sex\u0026#39;]) dataset[\u0026#39;Fare\u0026#39;] = dataset[\u0026#39;Fare\u0026#39;].fillna(0) # Train the Random Forest Model random_forest = RandomForestClassifier() X_train = train_df[[\u0026#39;Age\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Pclass\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;]] Y_train = train_df[\u0026#39;Survived\u0026#39;] random_forest.fit(X_train, Y_train) # Define and Deploy the Function client = Client(\u0026#39;http://localhost:9004/\u0026#39;) def titanic_survival_predictor(_arg1, _arg2, _arg3, _arg4, _arg5, _arg6): import pandas as pd # Get the new app\u0026#39;s data in a dictionary row = {\u0026#39;Age\u0026#39;: _arg1, \u0026#39;Sex\u0026#39;: _arg2, \u0026#39;Pclass\u0026#39;: _arg3, \u0026#39;Fare\u0026#39;: _arg4, \u0026#39;SibSp\u0026#39;: _arg5, \u0026#39;Parch\u0026#39;: _arg6} # Convert it into a DataFrame test_data = pd.DataFrame(data=row, index=[0]) # Predict the survival and death probabilities predprob_survival = random_forest.predict_proba(test_data) # Return only the survival probability return [probability[1] for probability in predprob_survival] client.deploy(\u0026#39;titanic_survival_predictor\u0026#39;, titanic_survival_predictor, \u0026#39;Predicts survival probability\u0026#39;, override=True) There is need to check the accuracy of the model built. Although, it is not the focus of the post but I will show a brief way of checking and interpreting the results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Define features and labels features = [\u0026#39;Age\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Pclass\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;] X = train_df[features] y = train_df[\u0026#39;Survived\u0026#39;] # Split the data (e.g., 80% train, 20% test) X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model on training set random_forest = RandomForestClassifier() random_forest.fit(X_train, y_train) # Predict on validation set y_pred = random_forest.predict(X_val) # Evaluate accuracy acc = accuracy_score(y_val, y_pred) print(f\u0026#34;Accuracy: {acc:.4f}\u0026#34;) # Optional: Confusion matrix and classification report print(\u0026#34;Confusion Matrix:\u0026#34;) print(confusion_matrix(y_val, y_pred)) print(\u0026#34;\\nClassification Report:\u0026#34;) print(classification_report(y_val, y_pred)) The result is as below:\nThe formula for accuracy is $$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\nTP: True Positives (Survived correctly predicted as survived)\nTN: True Negatives (Did not survive correctly predicted as not survived)\nFP: False Positives (Survived incorrectly predicted as not survived)\nFN: False Negatives (Did not survive incorrectly predicted as survived)\nNow, let\u0026rsquo;s understand the accuracy results.\nThe model is 81.6% accurate which is not perfect but reliable.\nTo know where the error occured, the confusion matrix function will diagnose our model.\nAs seen, 89 True Negative and 16 False Positives mean the model predicted 89 deaths correctly and 16 deaths incorrectly. The 17 False Negatives and 57 True Positives means the model predicted 17 deaths incorrectly and 57 survivors correctly.\nTo put it plainly, 89 deaths and 57 survivors were correctly predicted while 17 survivors and 16 deaths were inaccurately predicted.\nThere are several ways to perfect the model while avoiding overfitting. I might cover this in another project üôÇ.\nStep 3: I installed Tabpy by following the instructions on the tableau resources list below. This step is necessary to help tableau understand your script and correctly interprete it.\nFollow the instructions in the links below to connect Tableau Desktop and configure the analytics extension\nConfigure analytics extension TabPy Installation How To build advanced analytics Resources :\nBYOM Tableau guide Analytics Extensions settings The icons used were sourced from flaticons.com\nStep 4: Using jupyter IDE, I launched anaconda prompt after installing all the necessary libraries/packages and ran the command \u0026ldquo;tabpy\u0026rdquo; this will start listening to the port as seen below\nStep 5: Launch tableau, depending on the version you are using, hover on help, go to Settings and performance ‚Üí Manage Analytics Extensions connection.. and input parameters as seen below then click the test connection button.\nStep 6: Connect to the trained data source and write the script for the analysis. The tableau script for Death and survival prediction was written as in the image below :\nThe simple dashboard draft can be accessed here\nThis is also a short display of how the dashboard functions while in use\nFunctionality of the dashboard\r","permalink":"//localhost:1313/posts/first/","summary":"\u003cdiv style=\"text-align: justify; max-width: 700px; margin: auto;\"\u003e\r\n This project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck.\r\nWe all know the story behind the famous titanic ship. But just incase you don't know the story behind Titanic, let me give you a brief gist üòâ.\r\nTitanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time.\r\nTitanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers.\r\nTitanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier.\r\nOn the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised.\r\nOver about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\r\nLet's dive into the brief steps of how I handled this project using anaconda jupyter notebook and tableau. We will get the data, explore, process, and build the ML models. We will then depoly the model using TabPy function and see the results in tableau.\r\nYou can access the datasource here - [Titanic](https://www.kaggle.com/c/titanic) dataset on Kaggle. The demographic characteristics are the independent variables.\r\n\u003c/div\u003e\r\n\u003cp\u003eStep 1: I got data from Kaggle. You can access it here You can download the dataset \u003ca href=\"/files/train.csv\"\u003ehere\u003c/a\u003e.\n\u003cbr\u003e\nI performed some basic data exploration steps to review the data.\u003c/p\u003e","title":"Python to Tabpy - Tableau"},{"content":"\rThis project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database. The tools used for the project are listed below:\nJupyter Google console (console.cloud.google) Ga-dev-tools (ga-dev-tools) Snowflake (app.snowflake.com) I will give a brief summary of the steps involved in the project from start to finish.\nStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed. Step 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases. Step 3 : I connected to the GA4 developers console to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc. The basic steps are : Go to the Google Cloud Console (have the property id ready). Navigate to APIs \u0026amp; Services \u0026gt; Credentials. Click on Create Credentials and select OAuth 2.0 Client IDs. Configure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo; Next, run this script below to generate your access token. This will automatically download on the specified path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import os import json from google.auth.transport.requests import Request from google.oauth2.credentials import Credentials from google.analytics.data_v1beta import BetaAnalyticsDataClient # Path to your OAuth credentials JSON OAUTH_JSON_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\Oauth_Credentials.json\u0026#34; TOKEN_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\token.json\u0026#34; def get_credentials(): creds = None # Load existing credentials if available if os.path.exists(TOKEN_PATH): creds = Credentials.from_authorized_user_file(TOKEN_PATH) # If there are no valid credentials, authenticate using OAuth JSON if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) # Refresh token automatically else: raise Exception(\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;) # Save updated credentials (with refreshed token) with open(TOKEN_PATH, \u0026#34;w\u0026#34;) as token_file: token_file.write(creds.to_json()) return creds # Get authenticated credentials credentials = get_credentials() client = BetaAnalyticsDataClient(credentials=credentials) * Note: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors.\r- Step 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response. - Step 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available.\rThe snowflake connector needs to be installed by running.\r1 pip install snowflake-connector-python There\u0026rsquo;s also need to setup the following :\n- User:\n- Password:\n- Account:\n- Warehouse:\n- Database:\n- Schema:\nNote: GA4 default limit is 10000 rows. you can use pagination (limit and offset) to get more rows. Here is a snippet of the python code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # Google Analytics Property ID property_id = \u0026#34;12345687\u0026#34; # Date range Get previous day data after retrieving historical data start_date = \u0026#34;yesterday\u0026#34; end_date = \u0026#34;yesterday\u0026#34; # API request BATCH_SIZE = 10000 all_data = [] offset = 0 while True: request = RunReportRequest( property=f\u0026#34;properties/{property_id}\u0026#34;, date_ranges=[DateRange(start_date=start_date, end_date=end_date)], metrics=[ Metric(name=\u0026#34;sessions\u0026#34;), Metric(name=\u0026#34;totalRevenue\u0026#34;), Metric(name=\u0026#34;transactions\u0026#34;), Metric(name=\u0026#34;taxAmount\u0026#34;) ], dimensions=[ Dimension(name=\u0026#34;date\u0026#34;), Dimension(name=\u0026#34;transactionId\u0026#34;), Dimension(name=\u0026#34;sessionPrimaryChannelGroup\u0026#34;), Dimension(name=\u0026#34;deviceCategory\u0026#34;), Dimension(name=\u0026#34;country\u0026#34;), Dimension(name=\u0026#34;city\u0026#34;), Dimension(name=\u0026#34;region\u0026#34;) ], limit=BATCH_SIZE, offset=offset ) try: response = client.run_report(request) logging.info(f\u0026#34;Fetched {len(response.rows)} rows starting from offset {offset}.\u0026#34;) # If no rows are returned, break the loop if not response.rows: break # Append the retrieved data for row in response.rows: all_data.append( [value.value for value in row.dimension_values] + [float(value.value) if \u0026#39;.\u0026#39; in value.value else int(value.value) for value in row.metric_values] ) # Increase the offset for the next batch offset += BATCH_SIZE except Exception as e: logging.error(f\u0026#34;Error fetching GA data: {e}\u0026#34;, exc_info=True) break # Convert the collected data into a DataFrame columns = [ \u0026#34;EVENT_DATE\u0026#34;, \u0026#34;TRANSACTION_ID\u0026#34;, \u0026#34;SESSION_PRIMARY_CHANNEL_GROUP\u0026#34;, \u0026#34;DEVICE_CATEGORY\u0026#34;, \u0026#34;COUNTRY\u0026#34;, \u0026#34;CITY\u0026#34;, \u0026#34;REGION\u0026#34;, \u0026#34;SESSIONS\u0026#34;, \u0026#34;TOTAL_REVENUE\u0026#34;, \u0026#34;TRANSACTIONS\u0026#34;, \u0026#34;TAX_AMOUNT\u0026#34; ] df = pd.DataFrame(all_data, columns=columns) # Convert date format df[\u0026#34;EVENT_DATE\u0026#34;] = pd.to_datetime(df[\u0026#34;EVENT_DATE\u0026#34;], format=\u0026#34;%Y%m%d\u0026#34;).dt.strftime(\u0026#34;%Y-%m-%d\u0026#34;) # Generate composite key Step 6 : Get data from shopify using airbyte. For step by step instructions, consult the documentation here airbyte.com. Step 7 : Create data models on snowflake with dimensions and metrics using joins and unions where needed in separate views, connected tableau to the views, designed dashboard, and published to Tableau Online. The db Schema, tables, and views were created as seen below:\nThe tableau dashboard draft is seen below:\nThis was tweaked according to user requirement and deployed.\n","permalink":"//localhost:1313/posts/second/","summary":"\u003cdiv style=\"text-align: justify; max-width: 700px; margin: auto;\"\u003e\r\nThis project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database. \r\n\u003cp\u003eThe tools used for the project are listed below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJupyter\u003c/li\u003e\n\u003cli\u003eGoogle console (console.cloud.google)\u003c/li\u003e\n\u003cli\u003eGa-dev-tools (ga-dev-tools)\u003c/li\u003e\n\u003cli\u003eSnowflake (app.snowflake.com)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI will give a brief summary of the steps involved in the project from start to finish.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed.\u003c/li\u003e\n\u003cli\u003eStep 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases.\u003c/li\u003e\n\u003cli\u003eStep 3 : I connected to the GA4 developers \u003ca href=\"https://console.cloud.google.com/\"\u003econsole\u003c/a\u003e to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc.  \u003cbr\u003e\nThe basic steps are :\n\u003cul\u003e\n\u003cli\u003eGo to the Google Cloud Console (have the property id ready).\u003c/li\u003e\n\u003cli\u003eNavigate to APIs \u0026amp; Services \u0026gt; Credentials.\u003c/li\u003e\n\u003cli\u003eClick on Create Credentials and select OAuth 2.0 Client IDs.\u003c/li\u003e\n\u003cli\u003eConfigure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eNext, run this script below to generate your access token. This will automatically download on the specified path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\r\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e14\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e15\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e16\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e17\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e18\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e19\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e20\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e21\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e22\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e23\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e24\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e25\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e26\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e27\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e28\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e29\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e30\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e31\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e32\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e33\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejson\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.auth.transport.requests\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.oauth2.credentials\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.analytics.data_v1beta\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Path to your OAuth credentials JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eOAUTH_JSON_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eOauth_Credentials.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003etoken.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Load existing credentials if available\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_authorized_user_file\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# If there are no valid credentials, authenticate using OAuth JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexpired\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh_token\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Refresh token automatically\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eraise\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Save updated credentials (with refreshed token)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;w\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto_json\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Get authenticated credentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cdiv style=\"text-align: justify; max-width: 700px; margin: auto;\"\u003e\r\n* Note: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors.\r\n- Step 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response. \r\n- Step 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available.\r\nThe snowflake connector needs to be installed by running.\r\n\u003c/div\u003e\r\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"n\"\u003esnowflake\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003econnector\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003epython\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eThere\u0026rsquo;s also need to setup the following :\u003cbr\u003e\n- User:\u003cbr\u003e\n- Password:\u003cbr\u003e\n- Account:\u003cbr\u003e\n- Warehouse:\u003cbr\u003e\n- Database:\u003cbr\u003e\n- Schema:\u003c/p\u003e","title":"GA to Snowflake"},{"content":"\rThis project was an interesting one. The tools used for the project are listed below:\nJupyter Google console (console.cloud.google) Ga-dev-tools (ga-dev-tools) Snowflake (app.snowflake.com) I will give a brief summary of the steps involved in the project from start to finish.\nStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed. Step 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases. Step 3 : I connected to the GA4 developers console to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc. The basic steps are : Go to the Google Cloud Console (have the property id ready). Navigate to APIs \u0026amp; Services \u0026gt; Credentials. Click on Create Credentials and select OAuth 2.0 Client IDs. Configure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo; Next, run this script below to generate your access token. This will automatically download on the specified path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import os import json from google.auth.transport.requests import Request from google.oauth2.credentials import Credentials from google.analytics.data_v1beta import BetaAnalyticsDataClient # Path to your OAuth credentials JSON OAUTH_JSON_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\Oauth_Credentials.json\u0026#34; TOKEN_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\token.json\u0026#34; def get_credentials(): creds = None # Load existing credentials if available if os.path.exists(TOKEN_PATH): creds = Credentials.from_authorized_user_file(TOKEN_PATH) # If there are no valid credentials, authenticate using OAuth JSON if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) # Refresh token automatically else: raise Exception(\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;) # Save updated credentials (with refreshed token) with open(TOKEN_PATH, \u0026#34;w\u0026#34;) as token_file: token_file.write(creds.to_json()) return creds # Get authenticated credentials credentials = get_credentials() client = BetaAnalyticsDataClient(credentials=credentials) * Note: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors.\r- Step 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response. - Step 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available.\rThe snowflake connector needs to be installed by running.\r1 pip install snowflake-connector-python There\u0026rsquo;s also need to setup the following :\n- User:\n- Password:\n- Account:\n- Warehouse:\n- Database:\n- Schema:\nNote: GA4 default limit is 10000 rows. you can use pagination (limit and offset) to get more rows. Here is a snippet of the python code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # Google Analytics Property ID property_id = \u0026#34;12345687\u0026#34; # Date range Get previous day data after retrieving historical data start_date = \u0026#34;yesterday\u0026#34; end_date = \u0026#34;yesterday\u0026#34; # API request BATCH_SIZE = 10000 all_data = [] offset = 0 while True: request = RunReportRequest( property=f\u0026#34;properties/{property_id}\u0026#34;, date_ranges=[DateRange(start_date=start_date, end_date=end_date)], metrics=[ Metric(name=\u0026#34;sessions\u0026#34;), Metric(name=\u0026#34;totalRevenue\u0026#34;), Metric(name=\u0026#34;transactions\u0026#34;), Metric(name=\u0026#34;taxAmount\u0026#34;) ], dimensions=[ Dimension(name=\u0026#34;date\u0026#34;), Dimension(name=\u0026#34;transactionId\u0026#34;), Dimension(name=\u0026#34;sessionPrimaryChannelGroup\u0026#34;), Dimension(name=\u0026#34;deviceCategory\u0026#34;), Dimension(name=\u0026#34;country\u0026#34;), Dimension(name=\u0026#34;city\u0026#34;), Dimension(name=\u0026#34;region\u0026#34;) ], limit=BATCH_SIZE, offset=offset ) try: response = client.run_report(request) logging.info(f\u0026#34;Fetched {len(response.rows)} rows starting from offset {offset}.\u0026#34;) # If no rows are returned, break the loop if not response.rows: break # Append the retrieved data for row in response.rows: all_data.append( [value.value for value in row.dimension_values] + [float(value.value) if \u0026#39;.\u0026#39; in value.value else int(value.value) for value in row.metric_values] ) # Increase the offset for the next batch offset += BATCH_SIZE except Exception as e: logging.error(f\u0026#34;Error fetching GA data: {e}\u0026#34;, exc_info=True) break # Convert the collected data into a DataFrame columns = [ \u0026#34;EVENT_DATE\u0026#34;, \u0026#34;TRANSACTION_ID\u0026#34;, \u0026#34;SESSION_PRIMARY_CHANNEL_GROUP\u0026#34;, \u0026#34;DEVICE_CATEGORY\u0026#34;, \u0026#34;COUNTRY\u0026#34;, \u0026#34;CITY\u0026#34;, \u0026#34;REGION\u0026#34;, \u0026#34;SESSIONS\u0026#34;, \u0026#34;TOTAL_REVENUE\u0026#34;, \u0026#34;TRANSACTIONS\u0026#34;, \u0026#34;TAX_AMOUNT\u0026#34; ] df = pd.DataFrame(all_data, columns=columns) # Convert date format df[\u0026#34;EVENT_DATE\u0026#34;] = pd.to_datetime(df[\u0026#34;EVENT_DATE\u0026#34;], format=\u0026#34;%Y%m%d\u0026#34;).dt.strftime(\u0026#34;%Y-%m-%d\u0026#34;) # Generate composite key Step 6 : Get data from shopify using airbyte. For step by step instructions, consult the documentation here airbyte.com. Step 7 : Create data models on snowflake with dimensions and metrics using joins and unions where needed in separate views, connected tableau to the views, designed dashboard, and published to Tableau Online. The db Schema, tables, and views were created as seen below:\nThe tableau dashboard draft is seen below:\nThis was tweaked according to user requirement and deployed.\n","permalink":"//localhost:1313/posts/second/","summary":"\u003cdiv style=\"text-align: justify; max-width: 700px; margin: auto;\"\u003e\r\n\u003cp\u003eThis project was an interesting one. The tools used for the project are listed below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJupyter\u003c/li\u003e\n\u003cli\u003eGoogle console (console.cloud.google)\u003c/li\u003e\n\u003cli\u003eGa-dev-tools (ga-dev-tools)\u003c/li\u003e\n\u003cli\u003eSnowflake (app.snowflake.com)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI will give a brief summary of the steps involved in the project from start to finish.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed.\u003c/li\u003e\n\u003cli\u003eStep 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases.\u003c/li\u003e\n\u003cli\u003eStep 3 : I connected to the GA4 developers \u003ca href=\"https://console.cloud.google.com/\"\u003econsole\u003c/a\u003e to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc.  \u003cbr\u003e\nThe basic steps are :\n\u003cul\u003e\n\u003cli\u003eGo to the Google Cloud Console (have the property id ready).\u003c/li\u003e\n\u003cli\u003eNavigate to APIs \u0026amp; Services \u0026gt; Credentials.\u003c/li\u003e\n\u003cli\u003eClick on Create Credentials and select OAuth 2.0 Client IDs.\u003c/li\u003e\n\u003cli\u003eConfigure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eNext, run this script below to generate your access token. This will automatically download on the specified path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\r\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e14\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e15\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e16\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e17\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e18\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e19\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e20\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e21\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e22\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e23\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e24\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e25\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e26\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e27\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e28\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e29\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e30\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e31\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e32\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e33\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejson\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.auth.transport.requests\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.oauth2.credentials\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.analytics.data_v1beta\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Path to your OAuth credentials JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eOAUTH_JSON_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eOauth_Credentials.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003etoken.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Load existing credentials if available\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_authorized_user_file\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# If there are no valid credentials, authenticate using OAuth JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexpired\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh_token\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Refresh token automatically\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eraise\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Save updated credentials (with refreshed token)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;w\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto_json\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Get authenticated credentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cdiv style=\"text-align: justify; max-width: 700px; margin: auto;\"\u003e\r\n* Note: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors.\r\n- Step 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response. \r\n- Step 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available.\r\nThe snowflake connector needs to be installed by running.\r\n\u003c/div\u003e\r\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"n\"\u003esnowflake\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003econnector\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003epython\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eThere\u0026rsquo;s also need to setup the following :\u003cbr\u003e\n- User:\u003cbr\u003e\n- Password:\u003cbr\u003e\n- Account:\u003cbr\u003e\n- Warehouse:\u003cbr\u003e\n- Database:\u003cbr\u003e\n- Schema:\u003c/p\u003e","title":"GA to Snowflake"},{"content":"\rThis project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck.\rWe all know the story behind the famous titanic ship. But just incase you don't know the story behind Titanic, let me give you a brief gist üòâ.\rTitanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time.\rTitanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers.\rTitanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier.\rOn the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised.\rOver about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\rLet's dive into the brief steps of how I handled this project using anaconda jupyter notebook and tableau. We will get the data, explore, process, and build the ML models. We will then depoly the model using TabPy function and see the results in tableau.\rYou can access the datasource here - [Titanic](https://www.kaggle.com/c/titanic) dataset on Kaggle. The demographic characteristics are the independent variables.\rStep 1: I got data from Kaggle. You can access it here You can download the dataset here. I performed some basic data exploration steps to review the data.\n1 train_df.info() 1 train_df.describe() From this result, while other feature counts are 891, the count of age is 714 depicting missing data. 38% of the passengers out of the training set survived the titanic (mean - 0.38) with ages ranging from 0.4 to 8.\n1 train_df.head(10) There is need to convert most features to numeric so the model can process the features accurately.\nTo take a deeper look at the missing data, I ran this short code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Calculate missing values per column missing_total = train_df.isnull().sum() # Calculate percentage of missing values missing_percent = (missing_total / train_df.shape[0]) * 100 # Combine into a single DataFrame missing_data = pd.DataFrame({ \u0026#39;Total\u0026#39;: missing_total, \u0026#39;%\u0026#39;: missing_percent.round(1) }) # Sort by total missing values and show top 5 missing_data_sorted = missing_data.sort_values(by=\u0026#39;Total\u0026#39;, ascending=False) missing_data_sorted.head(5) To clean the data and fill missing columns, I used the part of the code block # Fill NAN values in Age with the random numbers generated in step 2\nThe result Step 2: There is need to create and deploy the function that can take the selected parameter values in Tableau as input and return the probability of a person surviving Titanic. I wrote the final Python code as seen below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # Importing the libraries import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder import tabpy_client from tabpy_client.client import Client # Loading the dataset train_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/train.csv\u0026#39;) test_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/test.csv\u0026#39;) # Dealing with missing values in the Age variable data = [train_df, test_df] for dataset in data: mean = train_df[\u0026#34;Age\u0026#34;].mean() std = train_df[\u0026#34;Age\u0026#34;].std() # Use train_df.std() for consistency is_null = dataset[\u0026#34;Age\u0026#34;].isnull().sum() # Compute random numbers between mean, std, and is_null rand_age = np.random.randint(mean - std, mean + std, size=is_null) # Fill NAN values in Age with the random numbers generated age_slice = dataset[\u0026#34;Age\u0026#34;].copy() age_slice[np.isnan(age_slice)] = rand_age dataset[\u0026#34;Age\u0026#34;] = age_slice dataset[\u0026#34;Age\u0026#34;] = train_df[\u0026#34;Age\u0026#34;].astype(int) # Define age bins and labels bins = [0, 11, 19, 27, 35, 43, 58, 66, np.inf] labels = [0, 1, 2, 3, 4, 5, 6, 7] # Apply pd.cut to categorize Age variable for dataset in data: dataset[\u0026#39;Age\u0026#39;] = pd.cut(dataset[\u0026#39;Age\u0026#39;], bins=bins, labels=labels, right=False).astype(int) #Encoding the categorical labels le = LabelEncoder() for dataset in data: dataset[\u0026#39;Sex\u0026#39;] = le.fit_transform(dataset[\u0026#39;Sex\u0026#39;]) dataset[\u0026#39;Fare\u0026#39;] = dataset[\u0026#39;Fare\u0026#39;].fillna(0) # Train the Random Forest Model random_forest = RandomForestClassifier() X_train = train_df[[\u0026#39;Age\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Pclass\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;]] Y_train = train_df[\u0026#39;Survived\u0026#39;] random_forest.fit(X_train, Y_train) # Define and Deploy the Function client = Client(\u0026#39;http://localhost:9004/\u0026#39;) def titanic_survival_predictor(_arg1, _arg2, _arg3, _arg4, _arg5, _arg6): import pandas as pd # Get the new app\u0026#39;s data in a dictionary row = {\u0026#39;Age\u0026#39;: _arg1, \u0026#39;Sex\u0026#39;: _arg2, \u0026#39;Pclass\u0026#39;: _arg3, \u0026#39;Fare\u0026#39;: _arg4, \u0026#39;SibSp\u0026#39;: _arg5, \u0026#39;Parch\u0026#39;: _arg6} # Convert it into a DataFrame test_data = pd.DataFrame(data=row, index=[0]) # Predict the survival and death probabilities predprob_survival = random_forest.predict_proba(test_data) # Return only the survival probability return [probability[1] for probability in predprob_survival] client.deploy(\u0026#39;titanic_survival_predictor\u0026#39;, titanic_survival_predictor, \u0026#39;Predicts survival probability\u0026#39;, override=True) There is need to check the accuracy of the model built. Although, it is not the focus of the post but I will show a brief way of checking and interpreting the results.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Define features and labels features = [\u0026#39;Age\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Pclass\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;] X = train_df[features] y = train_df[\u0026#39;Survived\u0026#39;] # Split the data (e.g., 80% train, 20% test) X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model on training set random_forest = RandomForestClassifier() random_forest.fit(X_train, y_train) # Predict on validation set y_pred = random_forest.predict(X_val) # Evaluate accuracy acc = accuracy_score(y_val, y_pred) print(f\u0026#34;Accuracy: {acc:.4f}\u0026#34;) # Optional: Confusion matrix and classification report print(\u0026#34;Confusion Matrix:\u0026#34;) print(confusion_matrix(y_val, y_pred)) print(\u0026#34;\\nClassification Report:\u0026#34;) print(classification_report(y_val, y_pred)) The result is as below:\nThe formula for accuracy is $$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\nTP: True Positives (Survived correctly predicted as survived)\nTN: True Negatives (Did not survive correctly predicted as not survived)\nFP: False Positives (Survived incorrectly predicted as not survived)\nFN: False Negatives (Did not survive incorrectly predicted as survived)\nNow, let\u0026rsquo;s understand the accuracy results.\nThe model is 81.6% accurate which is not perfect but reliable.\nTo know where the error occured, the confusion matrix function will diagnose our model.\nAs seen, 89 True Negative and 16 False Positives mean the model predicted 89 deaths correctly and 16 deaths incorrectly. The 17 False Negatives and 57 True Positives means the model predicted 17 deaths incorrectly and 57 survivors correctly.\nTo put it plainly, 89 deaths and 57 survivors were correctly predicted while 17 survivors and 16 deaths were inaccurately predicted.\nThere are several ways to perfect the model while avoiding overfitting. I might cover this in another project üôÇ.\nStep 3: I installed Tabpy by following the instructions on the tableau resources list below. This step is necessary to help tableau understand your script and correctly interprete it.\nFollow the instructions in the links below to connect Tableau Desktop and configure the analytics extension\nConfigure analytics extension TabPy Installation How To build advanced analytics Resources :\nBYOM Tableau guide Analytics Extensions settings The icons used were sourced from flaticons.com\nStep 4: Using jupyter IDE, I launched anaconda prompt after installing all the necessary libraries/packages and ran the command \u0026ldquo;tabpy\u0026rdquo; this will start listening to the port as seen below\nStep 5: Launch tableau, depending on the version you are using, hover on help, go to Settings and performance ‚Üí Manage Analytics Extensions connection.. and input parameters as seen below then click the test connection button.\nStep 6: Connect to the trained data source and write the script for the analysis. The tableau script for Death and survival prediction was written as in the image below :\nThe simple dashboard draft can be accessed here\nThis is also a short display of how the dashboard functions while in use\nFunctionality of the dashboard\r","permalink":"//localhost:1313/posts/first/","summary":"\u003cdiv style=\"text-align: justify; max-width: 700px; margin: auto;\"\u003e\r\n This project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck.\r\nWe all know the story behind the famous titanic ship. But just incase you don't know the story behind Titanic, let me give you a brief gist üòâ.\r\nTitanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time.\r\nTitanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers.\r\nTitanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier.\r\nOn the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised.\r\nOver about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\r\nLet's dive into the brief steps of how I handled this project using anaconda jupyter notebook and tableau. We will get the data, explore, process, and build the ML models. We will then depoly the model using TabPy function and see the results in tableau.\r\nYou can access the datasource here - [Titanic](https://www.kaggle.com/c/titanic) dataset on Kaggle. The demographic characteristics are the independent variables.\r\n\u003c/div\u003e\r\n\u003cp\u003eStep 1: I got data from Kaggle. You can access it here You can download the dataset \u003ca href=\"/files/train.csv\"\u003ehere\u003c/a\u003e.\n\u003cbr\u003e\nI performed some basic data exploration steps to review the data.\u003c/p\u003e","title":"Python to Tabpy - Tableau"}]