[{"content":"This project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck. We all know the story behind the famous titanic ship. But just incase you don\u0026rsquo;t know the story behind Titanic, let me give you a brief gist üòâ. Titanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time. Titanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers. Titanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier. On the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised. Over about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\nLet\u0026rsquo;s dive into the brief steps of how I handled this project using anaconda jupyter notebook and tableau. We will get the data, explore, process, and build the ML models. We will then depoly the model using TabPy function and see the results in tableau. You can access the datasource here - Titanic dataset on Kaggle. The demographic characteristics are the independent variables.\nstep 1: I got data from kaggle. Step 2: I wrote the python code as seen below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # Importing the libraries import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder import tabpy_client from tabpy_client.client import Client # Loading the dataset train_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/train.csv\u0026#39;) test_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/test.csv\u0026#39;) # Dealing with missing values in the Age variable data = [train_df, test_df] for dataset in data: mean = train_df[\u0026#34;Age\u0026#34;].mean() std = train_df[\u0026#34;Age\u0026#34;].std() # Use train_df.std() for consistency is_null = dataset[\u0026#34;Age\u0026#34;].isnull().sum() # Compute random numbers between mean, std, and is_null rand_age = np.random.randint(mean - std, mean + std, size=is_null) # Fill NAN values in Age with the random numbers generated age_slice = dataset[\u0026#34;Age\u0026#34;].copy() age_slice[np.isnan(age_slice)] = rand_age dataset[\u0026#34;Age\u0026#34;] = age_slice dataset[\u0026#34;Age\u0026#34;] = train_df[\u0026#34;Age\u0026#34;].astype(int) # Define age bins and labels bins = [0, 11, 19, 27, 35, 43, 58, 66, np.inf] labels = [0, 1, 2, 3, 4, 5, 6, 7] # Apply pd.cut to categorize Age variable for dataset in data: dataset[\u0026#39;Age\u0026#39;] = pd.cut(dataset[\u0026#39;Age\u0026#39;], bins=bins, labels=labels, right=False).astype(int) #Encoding the categorical labels le = LabelEncoder() for dataset in data: dataset[\u0026#39;Sex\u0026#39;] = le.fit_transform(dataset[\u0026#39;Sex\u0026#39;]) dataset[\u0026#39;Fare\u0026#39;] = dataset[\u0026#39;Fare\u0026#39;].fillna(0) # Train the Random Forest Model random_forest = RandomForestClassifier() X_train = train_df[[\u0026#39;Age\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Pclass\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;]] Y_train = train_df[\u0026#39;Survived\u0026#39;] random_forest.fit(X_train, Y_train) # Define and Deploy the Function client = Client(\u0026#39;http://localhost:9004/\u0026#39;) def titanic_survival_predictor(_arg1, _arg2, _arg3, _arg4, _arg5, _arg6): import pandas as pd # Get the new app\u0026#39;s data in a dictionary row = {\u0026#39;Age\u0026#39;: _arg1, \u0026#39;Sex\u0026#39;: _arg2, \u0026#39;Pclass\u0026#39;: _arg3, \u0026#39;Fare\u0026#39;: _arg4, \u0026#39;SibSp\u0026#39;: _arg5, \u0026#39;Parch\u0026#39;: _arg6} # Convert it into a DataFrame test_data = pd.DataFrame(data=row, index=[0]) # Predict the survival and death probabilities predprob_survival = random_forest.predict_proba(test_data) # Return only the survival probability return [probability[1] for probability in predprob_survival] client.deploy(\u0026#39;titanic_survival_predictor\u0026#39;, titanic_survival_predictor, \u0026#39;Predicts survival probability\u0026#39;, override=True) step 3: I installed Tabpy by following the instructions on the tableau resources list below. This step is necessary to help tableau understand your script and correctly interprete it.\nResources :\nhttps://www.tableau.com/developer/learning/bring-your-own-machine-learning-models-tableau-analytics-extensions-api https://help.tableau.com/current/server/en-us/config_r_tabpy.htm?_gl=11nb1q1a_gaOTA3NDY4OTk5LjE3MDcyNDI5NzM._ga_8YLN0SNXVS*MTczNzA1NzMzMy4yNzMuMS4xNzM3MDU4NTI5LjAuMC4w follow the instruction in this link below to connect Tableau Desktop and configure the analytics extension\nhttps://help.tableau.com/current/pro/desktop/en-us/r_connection_manage.htm https://tableau.github.io/TabPy/docs/server-install.html https://www.tableau.com/blog/building-advanced-analytics-applications-tabpy?_gl=1trnz20_gaOTA3NDY4OTk5LjE3MDcyNDI5NzM._ga_8YLN0SNXVS*MTczNzA2NTIyNi4yNzQuMS4xNzM3MDY1MjQyLjAuMC4w\u0026amp;_ga=2.10878772.183077179.1736784848-907468999.1707242973 The icons used were sourced from flaticons.com step 4: Using jupyter IDE, I launched anaconda prompt after installing all the necessary libraries/packages and ran the command \u0026ldquo;tabpy\u0026rdquo; this will start listening to the port as seen below\nStep 5: Launch tableau, depending on the version you are using, hover on help, Go to Settings ‚Üí Extensions and input parameters as seen below then click the test connection button\nstep 6: Connect to the trained data source and write the script for the analysis. The tableau script for Death and survival prediction was written as in the image below :\nThis is the simple dashboard draft :\nHere is also a short display of how the dashboard functions while in use\nFunctionality of the dashboard ","permalink":"//localhost:1313/posts/first/","summary":"\u003cp\u003eThis project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck.\nWe all know the story behind the famous titanic ship. But just incase you don\u0026rsquo;t know the story behind Titanic, let me give you a brief gist üòâ.\nTitanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time.\nTitanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers.\nTitanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier.\nOn the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised.\nOver about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\u003c/p\u003e","title":"First"},{"content":"This project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database.\nThe tools used for the project are listed below:\nJupyter Google console (console.cloud.google) Ga-dev-tools (ga-dev-tools) Snowflake I will give a brief summary of the steps involved in the project from start to finish.\nStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed. Step 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases. Step 3 : I connected to the GA4 developers console (https://console.cloud.google.com/) to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc. The basic steps are : Go to the Google Cloud Console (have the property id ready). Navigate to APIs \u0026amp; Services \u0026gt; Credentials. Click on Create Credentials and select OAuth 2.0 Client IDs. Configure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo; Next, run this script below to generate your access token. This will automatically download on the specified path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import os import json from google.auth.transport.requests import Request from google.oauth2.credentials import Credentials from google.analytics.data_v1beta import BetaAnalyticsDataClient # Path to your OAuth credentials JSON OAUTH_JSON_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\Oauth_Credentials.json\u0026#34; TOKEN_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\token.json\u0026#34; def get_credentials(): creds = None # Load existing credentials if available if os.path.exists(TOKEN_PATH): creds = Credentials.from_authorized_user_file(TOKEN_PATH) # If there are no valid credentials, authenticate using OAuth JSON if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) # Refresh token automatically else: raise Exception(\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;) # Save updated credentials (with refreshed token) with open(TOKEN_PATH, \u0026#34;w\u0026#34;) as token_file: token_file.write(creds.to_json()) return creds # Get authenticated credentials credentials = get_credentials() client = BetaAnalyticsDataClient(credentials=credentials) Note: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors. Step 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response. Step 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available. The snowflake connector needs to be installed by running. 1 pip install snowflake-connector-python There\u0026rsquo;s also need to setup the following : - User: - Password: - Account: - Warehouse: - Database: - Schema:\nNote: GA4 default limit is 10000 rows. you can use pagination (limit and offset) to get more rows. Here is a snippet of the python code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # Google Analytics Property ID property_id = \u0026#34;12345687\u0026#34; # Date range Get previous day data after retrieving historical data start_date = \u0026#34;yesterday\u0026#34; end_date = \u0026#34;yesterday\u0026#34; # API request BATCH_SIZE = 10000 all_data = [] offset = 0 while True: request = RunReportRequest( property=f\u0026#34;properties/{property_id}\u0026#34;, date_ranges=[DateRange(start_date=start_date, end_date=end_date)], metrics=[ Metric(name=\u0026#34;sessions\u0026#34;), Metric(name=\u0026#34;totalRevenue\u0026#34;), Metric(name=\u0026#34;transactions\u0026#34;), Metric(name=\u0026#34;taxAmount\u0026#34;) ], dimensions=[ Dimension(name=\u0026#34;date\u0026#34;), Dimension(name=\u0026#34;transactionId\u0026#34;), Dimension(name=\u0026#34;sessionPrimaryChannelGroup\u0026#34;), Dimension(name=\u0026#34;deviceCategory\u0026#34;), Dimension(name=\u0026#34;country\u0026#34;), Dimension(name=\u0026#34;city\u0026#34;), Dimension(name=\u0026#34;region\u0026#34;) ], limit=BATCH_SIZE, offset=offset ) try: response = client.run_report(request) logging.info(f\u0026#34;Fetched {len(response.rows)} rows starting from offset {offset}.\u0026#34;) # If no rows are returned, break the loop if not response.rows: break # Append the retrieved data for row in response.rows: all_data.append( [value.value for value in row.dimension_values] + [float(value.value) if \u0026#39;.\u0026#39; in value.value else int(value.value) for value in row.metric_values] ) # Increase the offset for the next batch offset += BATCH_SIZE except Exception as e: logging.error(f\u0026#34;Error fetching GA data: {e}\u0026#34;, exc_info=True) break # Convert the collected data into a DataFrame columns = [ \u0026#34;EVENT_DATE\u0026#34;, \u0026#34;TRANSACTION_ID\u0026#34;, \u0026#34;SESSION_PRIMARY_CHANNEL_GROUP\u0026#34;, \u0026#34;DEVICE_CATEGORY\u0026#34;, \u0026#34;COUNTRY\u0026#34;, \u0026#34;CITY\u0026#34;, \u0026#34;REGION\u0026#34;, \u0026#34;SESSIONS\u0026#34;, \u0026#34;TOTAL_REVENUE\u0026#34;, \u0026#34;TRANSACTIONS\u0026#34;, \u0026#34;TAX_AMOUNT\u0026#34; ] df = pd.DataFrame(all_data, columns=columns) # Convert date format df[\u0026#34;EVENT_DATE\u0026#34;] = pd.to_datetime(df[\u0026#34;EVENT_DATE\u0026#34;], format=\u0026#34;%Y%m%d\u0026#34;).dt.strftime(\u0026#34;%Y-%m-%d\u0026#34;) # Generate composite key Step 6 : Get data from shopify using airbyte. For step by step instructions, consult the documentation here (https://docs.airbyte.com/integrations/sources/shopify/) Step 7 : Create data models on snowflake with dimensions and metrics using joins and unions where needed in separate views, connected tableau to the views, designed dashboard, and published to Tableau Online. The db Schema, tables, and views were created as seen below:\nThe tableau dashboard draft is seen below:\nThis was tweaked according to user requirement and deployed.\n","permalink":"//localhost:1313/posts/second/","summary":"\u003cp\u003eThis project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database.\u003c/p\u003e\n\u003cp\u003eThe tools used for the project are listed below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJupyter\u003c/li\u003e\n\u003cli\u003eGoogle console (console.cloud.google)\u003c/li\u003e\n\u003cli\u003eGa-dev-tools (ga-dev-tools)\u003c/li\u003e\n\u003cli\u003eSnowflake\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI will give a brief summary of the steps involved in the project from start to finish.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed.\u003c/li\u003e\n\u003cli\u003eStep 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases.\u003c/li\u003e\n\u003cli\u003eStep 3 : I connected to the GA4 developers console (\u003ca href=\"https://console.cloud.google.com/\"\u003ehttps://console.cloud.google.com/\u003c/a\u003e) to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc.\nThe basic steps are :\n\u003cul\u003e\n\u003cli\u003eGo to the Google Cloud Console (have the property id ready).\u003c/li\u003e\n\u003cli\u003eNavigate to APIs \u0026amp; Services \u0026gt; Credentials.\u003c/li\u003e\n\u003cli\u003eClick on Create Credentials and select OAuth 2.0 Client IDs.\u003c/li\u003e\n\u003cli\u003eConfigure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eNext, run this script below to generate your access token. This will automatically download on the specified path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e14\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e15\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e16\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e17\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e18\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e19\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e20\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e21\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e22\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e23\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e24\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e25\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e26\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e27\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e28\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e29\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e30\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e31\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e32\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e33\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejson\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.auth.transport.requests\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.oauth2.credentials\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.analytics.data_v1beta\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Path to your OAuth credentials JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eOAUTH_JSON_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eOauth_Credentials.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003etoken.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Load existing credentials if available\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_authorized_user_file\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# If there are no valid credentials, authenticate using OAuth JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexpired\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh_token\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Refresh token automatically\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eraise\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Save updated credentials (with refreshed token)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;w\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto_json\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Get authenticated credentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eNote: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eStep 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response.\u003c/li\u003e\n\u003cli\u003eStep 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available.\nThe snowflake connector needs to be installed by running.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"n\"\u003esnowflake\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003econnector\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003epython\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eThere\u0026rsquo;s also need to setup the following :\n- User:\n- Password:\n- Account:\n- Warehouse:\n- Database:\n- Schema:\u003c/p\u003e","title":"GA to Snowflake"},{"content":"This project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck. We all know the story behind the famous titanic ship. But just incase you don\u0026rsquo;t know the story behind Titanic, let me give you a brief gist üòâ. Titanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time. Titanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers. Titanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier. On the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised. Over about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\nLet\u0026rsquo;s dive into the brief steps of how I handled this project using anaconda jupyter notebook and tableau. We will get the data, explore, process, and build the ML models. We will then depoly the model using TabPy function and see the results in tableau. You can access the datasource here - Titanic dataset on Kaggle. The demographic characteristics are the independent variables.\nstep 1: I got data from kaggle. Step 2: I wrote the python code as seen below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # Importing the libraries import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder import tabpy_client from tabpy_client.client import Client # Loading the dataset train_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/train.csv\u0026#39;) test_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/test.csv\u0026#39;) # Dealing with missing values in the Age variable data = [train_df, test_df] for dataset in data: mean = train_df[\u0026#34;Age\u0026#34;].mean() std = train_df[\u0026#34;Age\u0026#34;].std() # Use train_df.std() for consistency is_null = dataset[\u0026#34;Age\u0026#34;].isnull().sum() # Compute random numbers between mean, std, and is_null rand_age = np.random.randint(mean - std, mean + std, size=is_null) # Fill NAN values in Age with the random numbers generated age_slice = dataset[\u0026#34;Age\u0026#34;].copy() age_slice[np.isnan(age_slice)] = rand_age dataset[\u0026#34;Age\u0026#34;] = age_slice dataset[\u0026#34;Age\u0026#34;] = train_df[\u0026#34;Age\u0026#34;].astype(int) # Define age bins and labels bins = [0, 11, 19, 27, 35, 43, 58, 66, np.inf] labels = [0, 1, 2, 3, 4, 5, 6, 7] # Apply pd.cut to categorize Age variable for dataset in data: dataset[\u0026#39;Age\u0026#39;] = pd.cut(dataset[\u0026#39;Age\u0026#39;], bins=bins, labels=labels, right=False).astype(int) #Encoding the categorical labels le = LabelEncoder() for dataset in data: dataset[\u0026#39;Sex\u0026#39;] = le.fit_transform(dataset[\u0026#39;Sex\u0026#39;]) dataset[\u0026#39;Fare\u0026#39;] = dataset[\u0026#39;Fare\u0026#39;].fillna(0) # Train the Random Forest Model random_forest = RandomForestClassifier() X_train = train_df[[\u0026#39;Age\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Pclass\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;]] Y_train = train_df[\u0026#39;Survived\u0026#39;] random_forest.fit(X_train, Y_train) # Define and Deploy the Function client = Client(\u0026#39;http://localhost:9004/\u0026#39;) def titanic_survival_predictor(_arg1, _arg2, _arg3, _arg4, _arg5, _arg6): import pandas as pd # Get the new app\u0026#39;s data in a dictionary row = {\u0026#39;Age\u0026#39;: _arg1, \u0026#39;Sex\u0026#39;: _arg2, \u0026#39;Pclass\u0026#39;: _arg3, \u0026#39;Fare\u0026#39;: _arg4, \u0026#39;SibSp\u0026#39;: _arg5, \u0026#39;Parch\u0026#39;: _arg6} # Convert it into a DataFrame test_data = pd.DataFrame(data=row, index=[0]) # Predict the survival and death probabilities predprob_survival = random_forest.predict_proba(test_data) # Return only the survival probability return [probability[1] for probability in predprob_survival] client.deploy(\u0026#39;titanic_survival_predictor\u0026#39;, titanic_survival_predictor, \u0026#39;Predicts survival probability\u0026#39;, override=True) step 3: I installed Tabpy by following the instructions on the tableau resources list below. This step is necessary to help tableau understand your script and correctly interprete it.\nResources :\nhttps://www.tableau.com/developer/learning/bring-your-own-machine-learning-models-tableau-analytics-extensions-api https://help.tableau.com/current/server/en-us/config_r_tabpy.htm?_gl=11nb1q1a_gaOTA3NDY4OTk5LjE3MDcyNDI5NzM._ga_8YLN0SNXVS*MTczNzA1NzMzMy4yNzMuMS4xNzM3MDU4NTI5LjAuMC4w follow the instruction in this link below to connect Tableau Desktop and configure the analytics extension\nhttps://help.tableau.com/current/pro/desktop/en-us/r_connection_manage.htm https://tableau.github.io/TabPy/docs/server-install.html https://www.tableau.com/blog/building-advanced-analytics-applications-tabpy?_gl=1trnz20_gaOTA3NDY4OTk5LjE3MDcyNDI5NzM._ga_8YLN0SNXVS*MTczNzA2NTIyNi4yNzQuMS4xNzM3MDY1MjQyLjAuMC4w\u0026amp;_ga=2.10878772.183077179.1736784848-907468999.1707242973 The icons used were sourced from flaticons.com step 4: Using jupyter IDE, I launched anaconda prompt after installing all the necessary libraries/packages and ran the command \u0026ldquo;tabpy\u0026rdquo; this will start listening to the port as seen below\nStep 5: Launch tableau, depending on the version you are using, hover on help, Go to Settings ‚Üí Extensions and input parameters as seen below then click the test connection button\nstep 6: Connect to the trained data source and write the script for the analysis. The tableau script for Death and survival prediction was written as in the image below :\nThis is the simple dashboard draft :\nHere is also a short display of how the dashboard functions while in use\nFunctionality of the dashboard\r\u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD\r=======\n5e6b5d829f22fb1cee4bf57da8081fa6b5a55bae\n","permalink":"//localhost:1313/posts/first/","summary":"\u003cp\u003eThis project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck.\nWe all know the story behind the famous titanic ship. But just incase you don\u0026rsquo;t know the story behind Titanic, let me give you a brief gist üòâ.\nTitanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time.\nTitanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers.\nTitanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier.\nOn the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised.\nOver about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\u003c/p\u003e","title":"First"},{"content":"This project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database.\nThe tools used for the project are listed below:\nJupyter Google console (console.cloud.google) Ga-dev-tools (ga-dev-tools) Snowflake I will give a brief summary of the steps involved in the project from start to finish.\nStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed. Step 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases. Step 3 : I connected to the GA4 developers console (https://console.cloud.google.com/) to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc. The basic steps are : Go to the Google Cloud Console (have the property id ready). Navigate to APIs \u0026amp; Services \u0026gt; Credentials. Click on Create Credentials and select OAuth 2.0 Client IDs. Configure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo; Next, run this script below to generate your access token. This will automatically download on the specified path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import os import json from google.auth.transport.requests import Request from google.oauth2.credentials import Credentials from google.analytics.data_v1beta import BetaAnalyticsDataClient # Path to your OAuth credentials JSON OAUTH_JSON_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\Oauth_Credentials.json\u0026#34; TOKEN_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\token.json\u0026#34; def get_credentials(): creds = None # Load existing credentials if available if os.path.exists(TOKEN_PATH): creds = Credentials.from_authorized_user_file(TOKEN_PATH) # If there are no valid credentials, authenticate using OAuth JSON if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) # Refresh token automatically else: raise Exception(\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;) # Save updated credentials (with refreshed token) with open(TOKEN_PATH, \u0026#34;w\u0026#34;) as token_file: token_file.write(creds.to_json()) return creds # Get authenticated credentials credentials = get_credentials() client = BetaAnalyticsDataClient(credentials=credentials) Note: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors. Step 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response. Step 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available. The snowflake connector needs to be installed by running. 1 pip install snowflake-connector-python There\u0026rsquo;s also need to setup the following : - User: - Password: - Account: - Warehouse: - Database: - Schema:\nNote: GA4 default limit is 10000 rows. you can use pagination (limit and offset) to get more rows. Here is a snippet of the python code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # Google Analytics Property ID property_id = \u0026#34;12345687\u0026#34; # Date range Get previous day data after retrieving historical data start_date = \u0026#34;yesterday\u0026#34; end_date = \u0026#34;yesterday\u0026#34; # API request BATCH_SIZE = 10000 all_data = [] offset = 0 while True: request = RunReportRequest( property=f\u0026#34;properties/{property_id}\u0026#34;, date_ranges=[DateRange(start_date=start_date, end_date=end_date)], metrics=[ Metric(name=\u0026#34;sessions\u0026#34;), Metric(name=\u0026#34;totalRevenue\u0026#34;), Metric(name=\u0026#34;transactions\u0026#34;), Metric(name=\u0026#34;taxAmount\u0026#34;) ], dimensions=[ Dimension(name=\u0026#34;date\u0026#34;), Dimension(name=\u0026#34;transactionId\u0026#34;), Dimension(name=\u0026#34;sessionPrimaryChannelGroup\u0026#34;), Dimension(name=\u0026#34;deviceCategory\u0026#34;), Dimension(name=\u0026#34;country\u0026#34;), Dimension(name=\u0026#34;city\u0026#34;), Dimension(name=\u0026#34;region\u0026#34;) ], limit=BATCH_SIZE, offset=offset ) try: response = client.run_report(request) logging.info(f\u0026#34;Fetched {len(response.rows)} rows starting from offset {offset}.\u0026#34;) # If no rows are returned, break the loop if not response.rows: break # Append the retrieved data for row in response.rows: all_data.append( [value.value for value in row.dimension_values] + [float(value.value) if \u0026#39;.\u0026#39; in value.value else int(value.value) for value in row.metric_values] ) # Increase the offset for the next batch offset += BATCH_SIZE except Exception as e: logging.error(f\u0026#34;Error fetching GA data: {e}\u0026#34;, exc_info=True) break # Convert the collected data into a DataFrame columns = [ \u0026#34;EVENT_DATE\u0026#34;, \u0026#34;TRANSACTION_ID\u0026#34;, \u0026#34;SESSION_PRIMARY_CHANNEL_GROUP\u0026#34;, \u0026#34;DEVICE_CATEGORY\u0026#34;, \u0026#34;COUNTRY\u0026#34;, \u0026#34;CITY\u0026#34;, \u0026#34;REGION\u0026#34;, \u0026#34;SESSIONS\u0026#34;, \u0026#34;TOTAL_REVENUE\u0026#34;, \u0026#34;TRANSACTIONS\u0026#34;, \u0026#34;TAX_AMOUNT\u0026#34; ] df = pd.DataFrame(all_data, columns=columns) # Convert date format df[\u0026#34;EVENT_DATE\u0026#34;] = pd.to_datetime(df[\u0026#34;EVENT_DATE\u0026#34;], format=\u0026#34;%Y%m%d\u0026#34;).dt.strftime(\u0026#34;%Y-%m-%d\u0026#34;) # Generate composite key Step 6 : Get data from shopify using airbyte. For step by step instructions, consult the documentation here (https://docs.airbyte.com/integrations/sources/shopify/) Step 7 : Create data models on snowflake with dimensions and metrics using joins and unions where needed in separate views, connected tableau to the views, designed dashboard, and published to Tableau Online. The db Schema, tables, and views were created as seen below:\nThe tableau dashboard draft is seen below:\nThis was tweaked according to user requirement and deployed.\n","permalink":"//localhost:1313/posts/second/","summary":"\u003cp\u003eThis project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database.\u003c/p\u003e\n\u003cp\u003eThe tools used for the project are listed below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJupyter\u003c/li\u003e\n\u003cli\u003eGoogle console (console.cloud.google)\u003c/li\u003e\n\u003cli\u003eGa-dev-tools (ga-dev-tools)\u003c/li\u003e\n\u003cli\u003eSnowflake\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI will give a brief summary of the steps involved in the project from start to finish.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed.\u003c/li\u003e\n\u003cli\u003eStep 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases.\u003c/li\u003e\n\u003cli\u003eStep 3 : I connected to the GA4 developers console (\u003ca href=\"https://console.cloud.google.com/\"\u003ehttps://console.cloud.google.com/\u003c/a\u003e) to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc.\nThe basic steps are :\n\u003cul\u003e\n\u003cli\u003eGo to the Google Cloud Console (have the property id ready).\u003c/li\u003e\n\u003cli\u003eNavigate to APIs \u0026amp; Services \u0026gt; Credentials.\u003c/li\u003e\n\u003cli\u003eClick on Create Credentials and select OAuth 2.0 Client IDs.\u003c/li\u003e\n\u003cli\u003eConfigure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eNext, run this script below to generate your access token. This will automatically download on the specified path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e14\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e15\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e16\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e17\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e18\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e19\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e20\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e21\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e22\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e23\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e24\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e25\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e26\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e27\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e28\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e29\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e30\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e31\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e32\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e33\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejson\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.auth.transport.requests\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.oauth2.credentials\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.analytics.data_v1beta\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Path to your OAuth credentials JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eOAUTH_JSON_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eOauth_Credentials.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003etoken.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Load existing credentials if available\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_authorized_user_file\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# If there are no valid credentials, authenticate using OAuth JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexpired\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh_token\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Refresh token automatically\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eraise\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Save updated credentials (with refreshed token)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;w\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto_json\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Get authenticated credentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eNote: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eStep 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response.\u003c/li\u003e\n\u003cli\u003eStep 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available.\nThe snowflake connector needs to be installed by running.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"n\"\u003esnowflake\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003econnector\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003epython\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eThere\u0026rsquo;s also need to setup the following :\n- User:\n- Password:\n- Account:\n- Warehouse:\n- Database:\n- Schema:\u003c/p\u003e","title":"GA to Snowflake"},{"content":"This project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck. We all know the story behind the famous titanic ship. But just incase you don\u0026rsquo;t know the story behind Titanic, let me give you a brief gist üòâ. Titanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time. Titanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers. Titanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier. On the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised. Over about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\nLet\u0026rsquo;s dive into the brief steps of how I handled this project using anaconda jupyter notebook and tableau. We will get the data, explore, process, and build the ML models. We will then depoly the model using TabPy function and see the results in tableau. You can access the datasource here - Titanic dataset on Kaggle. The demographic characteristics are the independent variables.\nstep 1: I got data from kaggle. Step 2: I wrote the python code as seen below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # Importing the libraries import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder import tabpy_client from tabpy_client.client import Client # Loading the dataset train_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/train.csv\u0026#39;) test_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/test.csv\u0026#39;) # Dealing with missing values in the Age variable data = [train_df, test_df] for dataset in data: mean = train_df[\u0026#34;Age\u0026#34;].mean() std = train_df[\u0026#34;Age\u0026#34;].std() # Use train_df.std() for consistency is_null = dataset[\u0026#34;Age\u0026#34;].isnull().sum() # Compute random numbers between mean, std, and is_null rand_age = np.random.randint(mean - std, mean + std, size=is_null) # Fill NAN values in Age with the random numbers generated age_slice = dataset[\u0026#34;Age\u0026#34;].copy() age_slice[np.isnan(age_slice)] = rand_age dataset[\u0026#34;Age\u0026#34;] = age_slice dataset[\u0026#34;Age\u0026#34;] = train_df[\u0026#34;Age\u0026#34;].astype(int) # Define age bins and labels bins = [0, 11, 19, 27, 35, 43, 58, 66, np.inf] labels = [0, 1, 2, 3, 4, 5, 6, 7] # Apply pd.cut to categorize Age variable for dataset in data: dataset[\u0026#39;Age\u0026#39;] = pd.cut(dataset[\u0026#39;Age\u0026#39;], bins=bins, labels=labels, right=False).astype(int) #Encoding the categorical labels le = LabelEncoder() for dataset in data: dataset[\u0026#39;Sex\u0026#39;] = le.fit_transform(dataset[\u0026#39;Sex\u0026#39;]) dataset[\u0026#39;Fare\u0026#39;] = dataset[\u0026#39;Fare\u0026#39;].fillna(0) # Train the Random Forest Model random_forest = RandomForestClassifier() X_train = train_df[[\u0026#39;Age\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Pclass\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;]] Y_train = train_df[\u0026#39;Survived\u0026#39;] random_forest.fit(X_train, Y_train) # Define and Deploy the Function client = Client(\u0026#39;http://localhost:9004/\u0026#39;) def titanic_survival_predictor(_arg1, _arg2, _arg3, _arg4, _arg5, _arg6): import pandas as pd # Get the new app\u0026#39;s data in a dictionary row = {\u0026#39;Age\u0026#39;: _arg1, \u0026#39;Sex\u0026#39;: _arg2, \u0026#39;Pclass\u0026#39;: _arg3, \u0026#39;Fare\u0026#39;: _arg4, \u0026#39;SibSp\u0026#39;: _arg5, \u0026#39;Parch\u0026#39;: _arg6} # Convert it into a DataFrame test_data = pd.DataFrame(data=row, index=[0]) # Predict the survival and death probabilities predprob_survival = random_forest.predict_proba(test_data) # Return only the survival probability return [probability[1] for probability in predprob_survival] client.deploy(\u0026#39;titanic_survival_predictor\u0026#39;, titanic_survival_predictor, \u0026#39;Predicts survival probability\u0026#39;, override=True) step 3: I installed Tabpy by following the instructions on the tableau resources list below. This step is necessary to help tableau understand your script and correctly interprete it.\nResources :\nhttps://www.tableau.com/developer/learning/bring-your-own-machine-learning-models-tableau-analytics-extensions-api https://help.tableau.com/current/server/en-us/config_r_tabpy.htm?_gl=11nb1q1a_gaOTA3NDY4OTk5LjE3MDcyNDI5NzM._ga_8YLN0SNXVS*MTczNzA1NzMzMy4yNzMuMS4xNzM3MDU4NTI5LjAuMC4w follow the instruction in this link below to connect Tableau Desktop and configure the analytics extension\nhttps://help.tableau.com/current/pro/desktop/en-us/r_connection_manage.htm https://tableau.github.io/TabPy/docs/server-install.html https://www.tableau.com/blog/building-advanced-analytics-applications-tabpy?_gl=1trnz20_gaOTA3NDY4OTk5LjE3MDcyNDI5NzM._ga_8YLN0SNXVS*MTczNzA2NTIyNi4yNzQuMS4xNzM3MDY1MjQyLjAuMC4w\u0026amp;_ga=2.10878772.183077179.1736784848-907468999.1707242973 The icons used were sourced from flaticons.com step 4: Using jupyter IDE, I launched anaconda prompt after installing all the necessary libraries/packages and ran the command \u0026ldquo;tabpy\u0026rdquo; this will start listening to the port as seen below\nStep 5: Launch tableau, depending on the version you are using, hover on help, Go to Settings ‚Üí Extensions and input parameters as seen below then click the test connection button\nstep 6: Connect to the trained data source and write the script for the analysis. The tableau script for Death and survival prediction was written as in the image below :\nThis is the simple dashboard draft :\nHere is also a short display of how the dashboard functions while in use\nFunctionality of the dashboard\r","permalink":"//localhost:1313/posts/first/","summary":"\u003cp\u003eThis project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck.\nWe all know the story behind the famous titanic ship. But just incase you don\u0026rsquo;t know the story behind Titanic, let me give you a brief gist üòâ.\nTitanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time.\nTitanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers.\nTitanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier.\nOn the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised.\nOver about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\u003c/p\u003e","title":"First"},{"content":"This project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database.\nThe tools used for the project are listed below:\nJupyter Google console (console.cloud.google) Ga-dev-tools (ga-dev-tools) Snowflake I will give a brief summary of the steps involved in the project from start to finish.\nStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed. Step 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases. Step 3 : I connected to the GA4 developers console (https://console.cloud.google.com/) to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc. The basic steps are : Go to the Google Cloud Console (have the property id ready). Navigate to APIs \u0026amp; Services \u0026gt; Credentials. Click on Create Credentials and select OAuth 2.0 Client IDs. Configure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo; Next, run this script below to generate your access token. This will automatically download on the specified path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import os import json from google.auth.transport.requests import Request from google.oauth2.credentials import Credentials from google.analytics.data_v1beta import BetaAnalyticsDataClient # Path to your OAuth credentials JSON OAUTH_JSON_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\Oauth_Credentials.json\u0026#34; TOKEN_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\token.json\u0026#34; def get_credentials(): creds = None # Load existing credentials if available if os.path.exists(TOKEN_PATH): creds = Credentials.from_authorized_user_file(TOKEN_PATH) # If there are no valid credentials, authenticate using OAuth JSON if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) # Refresh token automatically else: raise Exception(\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;) # Save updated credentials (with refreshed token) with open(TOKEN_PATH, \u0026#34;w\u0026#34;) as token_file: token_file.write(creds.to_json()) return creds # Get authenticated credentials credentials = get_credentials() client = BetaAnalyticsDataClient(credentials=credentials) Note: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors. Step 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response. Step 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available. The snowflake connector needs to be installed by running. 1 pip install snowflake-connector-python There\u0026rsquo;s also need to setup the following : - User: - Password: - Account: - Warehouse: - Database: - Schema:\nNote: GA4 default limit is 10000 rows. you can use pagination (limit and offset) to get more rows. Here is a snippet of the python code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # Google Analytics Property ID property_id = \u0026#34;12345687\u0026#34; # Date range Get previous day data after retrieving historical data start_date = \u0026#34;yesterday\u0026#34; end_date = \u0026#34;yesterday\u0026#34; # API request BATCH_SIZE = 10000 all_data = [] offset = 0 while True: request = RunReportRequest( property=f\u0026#34;properties/{property_id}\u0026#34;, date_ranges=[DateRange(start_date=start_date, end_date=end_date)], metrics=[ Metric(name=\u0026#34;sessions\u0026#34;), Metric(name=\u0026#34;totalRevenue\u0026#34;), Metric(name=\u0026#34;transactions\u0026#34;), Metric(name=\u0026#34;taxAmount\u0026#34;) ], dimensions=[ Dimension(name=\u0026#34;date\u0026#34;), Dimension(name=\u0026#34;transactionId\u0026#34;), Dimension(name=\u0026#34;sessionPrimaryChannelGroup\u0026#34;), Dimension(name=\u0026#34;deviceCategory\u0026#34;), Dimension(name=\u0026#34;country\u0026#34;), Dimension(name=\u0026#34;city\u0026#34;), Dimension(name=\u0026#34;region\u0026#34;) ], limit=BATCH_SIZE, offset=offset ) try: response = client.run_report(request) logging.info(f\u0026#34;Fetched {len(response.rows)} rows starting from offset {offset}.\u0026#34;) # If no rows are returned, break the loop if not response.rows: break # Append the retrieved data for row in response.rows: all_data.append( [value.value for value in row.dimension_values] + [float(value.value) if \u0026#39;.\u0026#39; in value.value else int(value.value) for value in row.metric_values] ) # Increase the offset for the next batch offset += BATCH_SIZE except Exception as e: logging.error(f\u0026#34;Error fetching GA data: {e}\u0026#34;, exc_info=True) break # Convert the collected data into a DataFrame columns = [ \u0026#34;EVENT_DATE\u0026#34;, \u0026#34;TRANSACTION_ID\u0026#34;, \u0026#34;SESSION_PRIMARY_CHANNEL_GROUP\u0026#34;, \u0026#34;DEVICE_CATEGORY\u0026#34;, \u0026#34;COUNTRY\u0026#34;, \u0026#34;CITY\u0026#34;, \u0026#34;REGION\u0026#34;, \u0026#34;SESSIONS\u0026#34;, \u0026#34;TOTAL_REVENUE\u0026#34;, \u0026#34;TRANSACTIONS\u0026#34;, \u0026#34;TAX_AMOUNT\u0026#34; ] df = pd.DataFrame(all_data, columns=columns) # Convert date format df[\u0026#34;EVENT_DATE\u0026#34;] = pd.to_datetime(df[\u0026#34;EVENT_DATE\u0026#34;], format=\u0026#34;%Y%m%d\u0026#34;).dt.strftime(\u0026#34;%Y-%m-%d\u0026#34;) # Generate composite key Step 6 : Get data from shopify using airbyte. For step by step instructions, consult the documentation here (https://docs.airbyte.com/integrations/sources/shopify/) Step 7 : Create data models on snowflake with dimensions and metrics using joins and unions where needed in separate views, connected tableau to the views, designed dashboard, and published to Tableau Online. The db Schema, tables, and views were created as seen below:\nThe tableau dashboard draft is seen below:\nThis was tweaked according to user requirement and deployed.\n","permalink":"//localhost:1313/posts/second/","summary":"\u003cp\u003eThis project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database.\u003c/p\u003e\n\u003cp\u003eThe tools used for the project are listed below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJupyter\u003c/li\u003e\n\u003cli\u003eGoogle console (console.cloud.google)\u003c/li\u003e\n\u003cli\u003eGa-dev-tools (ga-dev-tools)\u003c/li\u003e\n\u003cli\u003eSnowflake\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI will give a brief summary of the steps involved in the project from start to finish.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed.\u003c/li\u003e\n\u003cli\u003eStep 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases.\u003c/li\u003e\n\u003cli\u003eStep 3 : I connected to the GA4 developers console (\u003ca href=\"https://console.cloud.google.com/\"\u003ehttps://console.cloud.google.com/\u003c/a\u003e) to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc.\nThe basic steps are :\n\u003cul\u003e\n\u003cli\u003eGo to the Google Cloud Console (have the property id ready).\u003c/li\u003e\n\u003cli\u003eNavigate to APIs \u0026amp; Services \u0026gt; Credentials.\u003c/li\u003e\n\u003cli\u003eClick on Create Credentials and select OAuth 2.0 Client IDs.\u003c/li\u003e\n\u003cli\u003eConfigure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eNext, run this script below to generate your access token. This will automatically download on the specified path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e14\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e15\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e16\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e17\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e18\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e19\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e20\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e21\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e22\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e23\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e24\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e25\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e26\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e27\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e28\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e29\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e30\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e31\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e32\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e33\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejson\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.auth.transport.requests\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.oauth2.credentials\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.analytics.data_v1beta\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Path to your OAuth credentials JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eOAUTH_JSON_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eOauth_Credentials.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003etoken.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Load existing credentials if available\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_authorized_user_file\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# If there are no valid credentials, authenticate using OAuth JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexpired\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh_token\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Refresh token automatically\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eraise\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Save updated credentials (with refreshed token)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;w\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto_json\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Get authenticated credentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eNote: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eStep 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response.\u003c/li\u003e\n\u003cli\u003eStep 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available.\nThe snowflake connector needs to be installed by running.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"n\"\u003esnowflake\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003econnector\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003epython\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eThere\u0026rsquo;s also need to setup the following :\n- User:\n- Password:\n- Account:\n- Warehouse:\n- Database:\n- Schema:\u003c/p\u003e","title":"GA to Snowflake"},{"content":"\rThis project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck.\rWe all know the story behind the famous titanic ship. But just incase you don't know the story behind Titanic, let me give you a brief gist üòâ.\rTitanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time.\rTitanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers.\rTitanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier.\rOn the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised.\rOver about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\rLet's dive into the brief steps of how I handled this project using anaconda jupyter notebook and tableau. We will get the data, explore, process, and build the ML models. We will then depoly the model using TabPy function and see the results in tableau.\rYou can access the datasource here - [Titanic](https://www.kaggle.com/c/titanic) dataset on Kaggle. The demographic characteristics are the independent variables.\rstep 1: I got data from kaggle.\nStep 2: I wrote the python code as seen below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # Importing the libraries import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder import tabpy_client from tabpy_client.client import Client # Loading the dataset train_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/train.csv\u0026#39;) test_df = pd.read_csv(\u0026#39;C:/Users/Documents/portfolio/test.csv\u0026#39;) # Dealing with missing values in the Age variable data = [train_df, test_df] for dataset in data: mean = train_df[\u0026#34;Age\u0026#34;].mean() std = train_df[\u0026#34;Age\u0026#34;].std() # Use train_df.std() for consistency is_null = dataset[\u0026#34;Age\u0026#34;].isnull().sum() # Compute random numbers between mean, std, and is_null rand_age = np.random.randint(mean - std, mean + std, size=is_null) # Fill NAN values in Age with the random numbers generated age_slice = dataset[\u0026#34;Age\u0026#34;].copy() age_slice[np.isnan(age_slice)] = rand_age dataset[\u0026#34;Age\u0026#34;] = age_slice dataset[\u0026#34;Age\u0026#34;] = train_df[\u0026#34;Age\u0026#34;].astype(int) # Define age bins and labels bins = [0, 11, 19, 27, 35, 43, 58, 66, np.inf] labels = [0, 1, 2, 3, 4, 5, 6, 7] # Apply pd.cut to categorize Age variable for dataset in data: dataset[\u0026#39;Age\u0026#39;] = pd.cut(dataset[\u0026#39;Age\u0026#39;], bins=bins, labels=labels, right=False).astype(int) #Encoding the categorical labels le = LabelEncoder() for dataset in data: dataset[\u0026#39;Sex\u0026#39;] = le.fit_transform(dataset[\u0026#39;Sex\u0026#39;]) dataset[\u0026#39;Fare\u0026#39;] = dataset[\u0026#39;Fare\u0026#39;].fillna(0) # Train the Random Forest Model random_forest = RandomForestClassifier() X_train = train_df[[\u0026#39;Age\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Pclass\u0026#39;, \u0026#39;Fare\u0026#39;, \u0026#39;SibSp\u0026#39;, \u0026#39;Parch\u0026#39;]] Y_train = train_df[\u0026#39;Survived\u0026#39;] random_forest.fit(X_train, Y_train) # Define and Deploy the Function client = Client(\u0026#39;http://localhost:9004/\u0026#39;) def titanic_survival_predictor(_arg1, _arg2, _arg3, _arg4, _arg5, _arg6): import pandas as pd # Get the new app\u0026#39;s data in a dictionary row = {\u0026#39;Age\u0026#39;: _arg1, \u0026#39;Sex\u0026#39;: _arg2, \u0026#39;Pclass\u0026#39;: _arg3, \u0026#39;Fare\u0026#39;: _arg4, \u0026#39;SibSp\u0026#39;: _arg5, \u0026#39;Parch\u0026#39;: _arg6} # Convert it into a DataFrame test_data = pd.DataFrame(data=row, index=[0]) # Predict the survival and death probabilities predprob_survival = random_forest.predict_proba(test_data) # Return only the survival probability return [probability[1] for probability in predprob_survival] client.deploy(\u0026#39;titanic_survival_predictor\u0026#39;, titanic_survival_predictor, \u0026#39;Predicts survival probability\u0026#39;, override=True) step 3: I installed Tabpy by following the instructions on the tableau resources list below. This step is necessary to help tableau understand your script and correctly interprete it.\nResources :\nhttps://www.tableau.com/developer/learning/bring-your-own-machine-learning-models-tableau-analytics-extensions-api https://help.tableau.com/current/server/en-us/config_r_tabpy.htm?_gl=11nb1q1a_gaOTA3NDY4OTk5LjE3MDcyNDI5NzM._ga_8YLN0SNXVS*MTczNzA1NzMzMy4yNzMuMS4xNzM3MDU4NTI5LjAuMC4w follow the instruction in this link below to connect Tableau Desktop and configure the analytics extension\nhttps://help.tableau.com/current/pro/desktop/en-us/r_connection_manage.htm https://tableau.github.io/TabPy/docs/server-install.html https://www.tableau.com/blog/building-advanced-analytics-applications-tabpy?_gl=1trnz20_gaOTA3NDY4OTk5LjE3MDcyNDI5NzM._ga_8YLN0SNXVS*MTczNzA2NTIyNi4yNzQuMS4xNzM3MDY1MjQyLjAuMC4w\u0026amp;_ga=2.10878772.183077179.1736784848-907468999.1707242973 The icons used were sourced from flaticons.com step 4: Using jupyter IDE, I launched anaconda prompt after installing all the necessary libraries/packages and ran the command \u0026ldquo;tabpy\u0026rdquo; this will start listening to the port as seen below\nStep 5: Launch tableau, depending on the version you are using, hover on help, Go to Settings ‚Üí Extensions and input parameters as seen below then click the test connection button\nstep 6: Connect to the trained data source and write the script for the analysis. The tableau script for Death and survival prediction was written as in the image below :\nThis is the simple dashboard draft :\nHere is also a short display of how the dashboard functions while in use\nFunctionality of the dashboard\r","permalink":"//localhost:1313/posts/first/","summary":"\u003cdiv style=\"text-align: justify; max-width: 700px; margin: auto;\"\u003e\r\n This project is one of the data science challenge posted on Kaggle and I decided to jump on it. It is about creating a predictive macine Learning model that predicts which passengers are likely to survive the titanic Shipwreck.\r\nWe all know the story behind the famous titanic ship. But just incase you don't know the story behind Titanic, let me give you a brief gist üòâ.\r\nTitanic was the largest ship afloat at the time of its completion in 1912. Designed to carry 2,435 passengers plus roughly 900 crew. Passengers were divided into three classes, reflecting the social stratification of the time.\r\nTitanic set sail from Southampton, England, on April 10, 1912, bound for New York City. It stopped briefly in Cherbourg, France, and Queenstown (now Cobh), Ireland, to pick up additional passengers.\r\nTitanic‚Äôs wireless operators received multiple iceberg warnings from other ships. Although, the crew was aware of ice in the vicinity, they maintained speed hoping to arrive earlier.\r\nOn the night of April 14, 1912, at around 11:40 p.m. ship‚Äôs time, despite evasive maneuvers, Titanic scraped along the iceberg‚Äôs starboard side causing punctures below the waterline. Five of the ship‚Äôs watertight compartments were compromised.\r\nOver about two hours and forty minutes, the bow sank deeper until the stern rose out of the water. An estimated 1500-1517. A disproportionate number of third-class passengers and crew lost their lives compared to first-class passengers, reflecting class-based access to lifeboats. Around 705 people survived. Many were taken aboard the Cunard liner Carpathia, which arrived at the scene after receiving Titanic‚Äôs distress signals.\r\nLet's dive into the brief steps of how I handled this project using anaconda jupyter notebook and tableau. We will get the data, explore, process, and build the ML models. We will then depoly the model using TabPy function and see the results in tableau.\r\nYou can access the datasource here - [Titanic](https://www.kaggle.com/c/titanic) dataset on Kaggle. The demographic characteristics are the independent variables.\r\n\u003c/div\u003e\r\n\u003cp\u003estep 1: I got data from kaggle.\u003c/p\u003e","title":"First"},{"content":"This project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database.\nThe tools used for the project are listed below:\nJupyter Google console (console.cloud.google) Ga-dev-tools (ga-dev-tools) Snowflake I will give a brief summary of the steps involved in the project from start to finish.\nStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed. Step 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases. Step 3 : I connected to the GA4 developers console (https://console.cloud.google.com/) to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc. The basic steps are : Go to the Google Cloud Console (have the property id ready). Navigate to APIs \u0026amp; Services \u0026gt; Credentials. Click on Create Credentials and select OAuth 2.0 Client IDs. Configure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo; Next, run this script below to generate your access token. This will automatically download on the specified path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import os import json from google.auth.transport.requests import Request from google.oauth2.credentials import Credentials from google.analytics.data_v1beta import BetaAnalyticsDataClient # Path to your OAuth credentials JSON OAUTH_JSON_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\Oauth_Credentials.json\u0026#34; TOKEN_PATH = \u0026#34;C:\\\\Users\\\\Downloads\\\\Python\\\\token.json\u0026#34; def get_credentials(): creds = None # Load existing credentials if available if os.path.exists(TOKEN_PATH): creds = Credentials.from_authorized_user_file(TOKEN_PATH) # If there are no valid credentials, authenticate using OAuth JSON if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) # Refresh token automatically else: raise Exception(\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;) # Save updated credentials (with refreshed token) with open(TOKEN_PATH, \u0026#34;w\u0026#34;) as token_file: token_file.write(creds.to_json()) return creds # Get authenticated credentials credentials = get_credentials() client = BetaAnalyticsDataClient(credentials=credentials) Note: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors. Step 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response. Step 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available. The snowflake connector needs to be installed by running. 1 pip install snowflake-connector-python There\u0026rsquo;s also need to setup the following : - User: - Password: - Account: - Warehouse: - Database: - Schema:\nNote: GA4 default limit is 10000 rows. you can use pagination (limit and offset) to get more rows. Here is a snippet of the python code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # Google Analytics Property ID property_id = \u0026#34;12345687\u0026#34; # Date range Get previous day data after retrieving historical data start_date = \u0026#34;yesterday\u0026#34; end_date = \u0026#34;yesterday\u0026#34; # API request BATCH_SIZE = 10000 all_data = [] offset = 0 while True: request = RunReportRequest( property=f\u0026#34;properties/{property_id}\u0026#34;, date_ranges=[DateRange(start_date=start_date, end_date=end_date)], metrics=[ Metric(name=\u0026#34;sessions\u0026#34;), Metric(name=\u0026#34;totalRevenue\u0026#34;), Metric(name=\u0026#34;transactions\u0026#34;), Metric(name=\u0026#34;taxAmount\u0026#34;) ], dimensions=[ Dimension(name=\u0026#34;date\u0026#34;), Dimension(name=\u0026#34;transactionId\u0026#34;), Dimension(name=\u0026#34;sessionPrimaryChannelGroup\u0026#34;), Dimension(name=\u0026#34;deviceCategory\u0026#34;), Dimension(name=\u0026#34;country\u0026#34;), Dimension(name=\u0026#34;city\u0026#34;), Dimension(name=\u0026#34;region\u0026#34;) ], limit=BATCH_SIZE, offset=offset ) try: response = client.run_report(request) logging.info(f\u0026#34;Fetched {len(response.rows)} rows starting from offset {offset}.\u0026#34;) # If no rows are returned, break the loop if not response.rows: break # Append the retrieved data for row in response.rows: all_data.append( [value.value for value in row.dimension_values] + [float(value.value) if \u0026#39;.\u0026#39; in value.value else int(value.value) for value in row.metric_values] ) # Increase the offset for the next batch offset += BATCH_SIZE except Exception as e: logging.error(f\u0026#34;Error fetching GA data: {e}\u0026#34;, exc_info=True) break # Convert the collected data into a DataFrame columns = [ \u0026#34;EVENT_DATE\u0026#34;, \u0026#34;TRANSACTION_ID\u0026#34;, \u0026#34;SESSION_PRIMARY_CHANNEL_GROUP\u0026#34;, \u0026#34;DEVICE_CATEGORY\u0026#34;, \u0026#34;COUNTRY\u0026#34;, \u0026#34;CITY\u0026#34;, \u0026#34;REGION\u0026#34;, \u0026#34;SESSIONS\u0026#34;, \u0026#34;TOTAL_REVENUE\u0026#34;, \u0026#34;TRANSACTIONS\u0026#34;, \u0026#34;TAX_AMOUNT\u0026#34; ] df = pd.DataFrame(all_data, columns=columns) # Convert date format df[\u0026#34;EVENT_DATE\u0026#34;] = pd.to_datetime(df[\u0026#34;EVENT_DATE\u0026#34;], format=\u0026#34;%Y%m%d\u0026#34;).dt.strftime(\u0026#34;%Y-%m-%d\u0026#34;) # Generate composite key Step 6 : Get data from shopify using airbyte. For step by step instructions, consult the documentation here (https://docs.airbyte.com/integrations/sources/shopify/) Step 7 : Create data models on snowflake with dimensions and metrics using joins and unions where needed in separate views, connected tableau to the views, designed dashboard, and published to Tableau Online. The db Schema, tables, and views were created as seen below:\nThe tableau dashboard draft is seen below:\nThis was tweaked according to user requirement and deployed.\n","permalink":"//localhost:1313/posts/second/","summary":"\u003cp\u003eThis project was an interesting one. The requirement(s) was to build an Ecommerce dashboard with data sources from Shopify, Google Analytics, and local database.\u003c/p\u003e\n\u003cp\u003eThe tools used for the project are listed below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJupyter\u003c/li\u003e\n\u003cli\u003eGoogle console (console.cloud.google)\u003c/li\u003e\n\u003cli\u003eGa-dev-tools (ga-dev-tools)\u003c/li\u003e\n\u003cli\u003eSnowflake\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI will give a brief summary of the steps involved in the project from start to finish.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStep 1 : I was to reveiw the front end of these applications and identify the key metrics that were needed.\u003c/li\u003e\n\u003cli\u003eStep 2 : After identifying these data sources, a plan was drafted on how to get these data out of the applications. The data was gotten from the shopify api, google analytics api 4 (GA4), oracle, and snowflake databases.\u003c/li\u003e\n\u003cli\u003eStep 3 : I connected to the GA4 developers console (\u003ca href=\"https://console.cloud.google.com/\"\u003ehttps://console.cloud.google.com/\u003c/a\u003e) to create my method of authentication. I used Oauth 2.0 client IDs for my authentication since I didn\u0026rsquo;t have a service account and running this on my pc.\nThe basic steps are :\n\u003cul\u003e\n\u003cli\u003eGo to the Google Cloud Console (have the property id ready).\u003c/li\u003e\n\u003cli\u003eNavigate to APIs \u0026amp; Services \u0026gt; Credentials.\u003c/li\u003e\n\u003cli\u003eClick on Create Credentials and select OAuth 2.0 Client IDs.\u003c/li\u003e\n\u003cli\u003eConfigure the consent screen and download the JSON file containing your client ID and client secret. \u0026ldquo;Don\u0026rsquo;t forget to Set the default Ips in the redirect URIs üôÇ.\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eNext, run this script below to generate your access token. This will automatically download on the specified path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e14\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e15\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e16\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e17\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e18\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e19\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e20\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e21\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e22\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e23\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e24\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e25\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e26\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e27\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e28\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e29\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e30\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e31\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e32\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e33\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejson\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.auth.transport.requests\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.oauth2.credentials\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003egoogle.analytics.data_v1beta\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Path to your OAuth credentials JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eOAUTH_JSON_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eOauth_Credentials.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;C:\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eUsers\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003eDownloads\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003ePython\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003etoken.json\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Load existing credentials if available\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_authorized_user_file\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# If there are no valid credentials, authenticate using OAuth JSON\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexpired\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh_token\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erefresh\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eRequest\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Refresh token automatically\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eraise\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;‚ùå OAuth credentials have expired or are missing. Please authenticate manually first.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Save updated credentials (with refreshed token)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTOKEN_PATH\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;w\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003etoken_file\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto_json\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecreds\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Get authenticated credentials\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eget_credentials\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eBetaAnalyticsDataClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ecredentials\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eNote: If yor python environment is having outdated widgets especially using jupyter, this could cause some errors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eStep 4 : I used the dev tools query explorer to select the dimensions and metrics I needed and ran the request to see the response.\u003c/li\u003e\n\u003cli\u003eStep 5 : I wrote my script and kept modifying until completed. This script connected to snowflake; extracted, transformed some fields, and Loaded data into the already created snowflake table. After these, next is to create a task for daily data update. Amongst the multiple ways to update data, I opted for windows task scheduler. I found this option more convenient considering the resources I have available.\nThe snowflake connector needs to be installed by running.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"n\"\u003esnowflake\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003econnector\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003epython\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eThere\u0026rsquo;s also need to setup the following :\n- User:\n- Password:\n- Account:\n- Warehouse:\n- Database:\n- Schema:\u003c/p\u003e","title":"GA to Snowflake"}]